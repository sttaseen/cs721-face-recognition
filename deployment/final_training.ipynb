{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f15792b-c8ad-450c-8ccd-5be72664b7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mappings saved to class_mappings.json\n",
      "Fold 1/5\n",
      "Fold 1, Epoch 1/10 - Loss: 1.9437, Accuracy: 0.3808\n",
      "Fold 1, Epoch 2/10 - Loss: 1.5665, Accuracy: 0.4812\n",
      "Fold 1, Epoch 3/10 - Loss: 1.3129, Accuracy: 0.6059\n",
      "Fold 1, Epoch 4/10 - Loss: 1.1168, Accuracy: 0.6494\n",
      "Fold 1, Epoch 5/10 - Loss: 0.9416, Accuracy: 0.6837\n",
      "Fold 1, Epoch 6/10 - Loss: 0.8521, Accuracy: 0.7180\n",
      "Fold 1, Epoch 7/10 - Loss: 0.7962, Accuracy: 0.7331\n",
      "Fold 1, Epoch 8/10 - Loss: 0.6868, Accuracy: 0.7824\n",
      "Fold 1, Epoch 9/10 - Loss: 0.6474, Accuracy: 0.7983\n",
      "Fold 1, Epoch 10/10 - Loss: 0.5436, Accuracy: 0.8142\n",
      "Fold 1 Validation Results:\n",
      "Test Loss: 0.6930, Test Accuracy: 0.7625, F1 Score: 0.7495\n",
      "Fold 2/5\n",
      "Fold 2, Epoch 1/10 - Loss: 2.0189, Accuracy: 0.3573\n",
      "Fold 2, Epoch 2/10 - Loss: 1.6264, Accuracy: 0.4678\n",
      "Fold 2, Epoch 3/10 - Loss: 1.3810, Accuracy: 0.5607\n",
      "Fold 2, Epoch 4/10 - Loss: 1.1310, Accuracy: 0.6444\n",
      "Fold 2, Epoch 5/10 - Loss: 1.0028, Accuracy: 0.6644\n",
      "Fold 2, Epoch 6/10 - Loss: 0.8531, Accuracy: 0.7138\n",
      "Fold 2, Epoch 7/10 - Loss: 0.7962, Accuracy: 0.7423\n",
      "Fold 2, Epoch 8/10 - Loss: 0.7369, Accuracy: 0.7590\n",
      "Fold 2, Epoch 9/10 - Loss: 0.6795, Accuracy: 0.7833\n",
      "Fold 2, Epoch 10/10 - Loss: 0.5979, Accuracy: 0.7983\n",
      "Fold 2 Validation Results:\n",
      "Test Loss: 0.6354, Test Accuracy: 0.7793, F1 Score: 0.7709\n",
      "Fold 3/5\n",
      "Fold 3, Epoch 1/10 - Loss: 2.0193, Accuracy: 0.3372\n",
      "Fold 3, Epoch 2/10 - Loss: 1.5817, Accuracy: 0.4879\n",
      "Fold 3, Epoch 3/10 - Loss: 1.3018, Accuracy: 0.5874\n",
      "Fold 3, Epoch 4/10 - Loss: 1.0983, Accuracy: 0.6586\n",
      "Fold 3, Epoch 5/10 - Loss: 0.9488, Accuracy: 0.6778\n",
      "Fold 3, Epoch 6/10 - Loss: 0.8328, Accuracy: 0.7013\n",
      "Fold 3, Epoch 7/10 - Loss: 0.7461, Accuracy: 0.7515\n",
      "Fold 3, Epoch 8/10 - Loss: 0.7359, Accuracy: 0.7598\n",
      "Fold 3, Epoch 9/10 - Loss: 0.6466, Accuracy: 0.7925\n",
      "Fold 3, Epoch 10/10 - Loss: 0.5358, Accuracy: 0.8243\n",
      "Fold 3 Validation Results:\n",
      "Test Loss: 0.6729, Test Accuracy: 0.7692, F1 Score: 0.7640\n",
      "Fold 4/5\n",
      "Fold 4, Epoch 1/10 - Loss: 2.0009, Accuracy: 0.3582\n",
      "Fold 4, Epoch 2/10 - Loss: 1.6246, Accuracy: 0.4703\n",
      "Fold 4, Epoch 3/10 - Loss: 1.3756, Accuracy: 0.5766\n",
      "Fold 4, Epoch 4/10 - Loss: 1.1730, Accuracy: 0.6301\n",
      "Fold 4, Epoch 5/10 - Loss: 1.0391, Accuracy: 0.6586\n",
      "Fold 4, Epoch 6/10 - Loss: 0.8884, Accuracy: 0.7088\n",
      "Fold 4, Epoch 7/10 - Loss: 0.8168, Accuracy: 0.7280\n",
      "Fold 4, Epoch 8/10 - Loss: 0.7444, Accuracy: 0.7464\n",
      "Fold 4, Epoch 9/10 - Loss: 0.6898, Accuracy: 0.7732\n",
      "Fold 4, Epoch 10/10 - Loss: 0.5940, Accuracy: 0.8033\n",
      "Fold 4 Validation Results:\n",
      "Test Loss: 0.6127, Test Accuracy: 0.7860, F1 Score: 0.7791\n",
      "Fold 5/5\n",
      "Fold 5, Epoch 1/10 - Loss: 2.0049, Accuracy: 0.3654\n",
      "Fold 5, Epoch 2/10 - Loss: 1.5959, Accuracy: 0.4799\n",
      "Fold 5, Epoch 3/10 - Loss: 1.3417, Accuracy: 0.5736\n",
      "Fold 5, Epoch 4/10 - Loss: 1.1430, Accuracy: 0.6304\n",
      "Fold 5, Epoch 5/10 - Loss: 0.9605, Accuracy: 0.6873\n",
      "Fold 5, Epoch 6/10 - Loss: 0.8726, Accuracy: 0.7082\n",
      "Fold 5, Epoch 7/10 - Loss: 0.7796, Accuracy: 0.7349\n",
      "Fold 5, Epoch 8/10 - Loss: 0.7054, Accuracy: 0.7701\n",
      "Fold 5, Epoch 9/10 - Loss: 0.6443, Accuracy: 0.7768\n",
      "Fold 5, Epoch 10/10 - Loss: 0.6211, Accuracy: 0.8027\n",
      "Fold 5 Validation Results:\n",
      "Test Loss: 0.6544, Test Accuracy: 0.7852, F1 Score: 0.7716\n",
      "Cross-validation completed and final model saved as 'final_resnet34_cv.pth'.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Store the class mappings\n",
    "def store_class_mappings(dataset, file_path):\n",
    "    class_mappings = {i: cls for i, cls in enumerate(dataset.classes)}\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(class_mappings, f)\n",
    "    print(f\"Class mappings saved to {file_path}\")\n",
    "\n",
    "# Set the path for class mappings\n",
    "class_mappings_path = 'class_mappings.json'\n",
    "\n",
    "# Create the ResNet34 model and load pretrained weights, excluding the final fc layer\n",
    "def create_resnet_model(num_classes, pretrained_path=None):\n",
    "    model = models.resnet34(pretrained=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)  # Replace the final layer for the new task\n",
    "    \n",
    "    if pretrained_path:\n",
    "        # Load the pretrained model, but exclude the final fully connected layer weights\n",
    "        state_dict = torch.load(pretrained_path)\n",
    "        # Remove the fc layer weights from the pretrained model\n",
    "        del state_dict['fc.weight']\n",
    "        del state_dict['fc.bias']\n",
    "        # Load the remaining layers\n",
    "        model.load_state_dict(state_dict, strict=False)  # strict=False ignores missing keys\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "# Define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.43636918, 0.38563913, 0.34477144],\n",
    "                         std=[0.29639485, 0.2698132, 0.26158142])\n",
    "])\n",
    "\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "#     transforms.RandomRotation(degrees=15),\n",
    "#     transforms.RandomCrop(size=(224, 224), padding=4),\n",
    "#     transforms.RandomVerticalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.43636918, 0.38563913, 0.34477144],\n",
    "#                          std=[0.29639485, 0.2698132, 0.26158142])\n",
    "# ])\n",
    "\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.43636918, 0.38563913, 0.34477144],\n",
    "                         std=[0.29639485, 0.2698132, 0.26158142])\n",
    "])\n",
    "\n",
    "# Set the dataset folder\n",
    "dataset_folder = '../data/lfw/'\n",
    "\n",
    "# Load datasets\n",
    "dataset = datasets.ImageFolder(root=dataset_folder, transform=train_transform)\n",
    "\n",
    "# Save class mappings after loading the dataset\n",
    "store_class_mappings(dataset, class_mappings_path)\n",
    "\n",
    "# Get the number of classes\n",
    "num_classes = len(dataset.classes)\n",
    "\n",
    "# Path to the pretrained model (.pth file)\n",
    "pretrained_model_path = './resnet_34_pretrained.pth'  # Replace with the correct path\n",
    "\n",
    "# KFold cross-validator\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to train and evaluate the model on a single fold\n",
    "def train_and_evaluate_fold(train_idx, val_idx, dataset, fold):\n",
    "    # Create subset for training and validation from indices\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "    # Create DataLoader for train and validation sets\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Apply validation transforms directly to the validation DataLoader\n",
    "    val_loader.dataset.transform = val_transform\n",
    "\n",
    "    # Initialize model, loss function, and optimizer for each fold\n",
    "    model = create_resnet_model(num_classes, pretrained_path=pretrained_model_path)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop for 20 epochs for this fold\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            correct_preds += (outputs.argmax(1) == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = correct_preds / len(train_loader.dataset)\n",
    "\n",
    "        print(f'Fold {fold+1}, Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "    \n",
    "    # Evaluate the model on the validation set (test equivalent)\n",
    "    print(f'Fold {fold+1} Validation Results:')\n",
    "    evaluate_model(model, val_loader)\n",
    "    return model\n",
    "\n",
    "# Test loop for evaluation on validation set\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for faster inference\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            preds = outputs.argmax(1)\n",
    "            correct_preds += (preds == labels).sum().item()\n",
    "            \n",
    "            # Store predictions and labels for further analysis\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate test loss and accuracy\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = correct_preds / len(test_loader.dataset)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f'Fold {fold+1}/{num_folds}')\n",
    "    model = train_and_evaluate_fold(train_idx, val_idx, dataset, fold)\n",
    "\n",
    "# Save the final trained model after cross-validation\n",
    "torch.save(model.state_dict(), 'final_resnet34_cv.pth')\n",
    "print(\"Cross-validation completed and final model saved as 'final_resnet34_cv.pth'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs721",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
